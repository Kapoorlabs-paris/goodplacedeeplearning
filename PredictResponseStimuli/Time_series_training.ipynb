{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict response to stimuli: lstm for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a *lstm timeseries* problem, we aim to predict the response of a stimuli, as a time series data. The input is a stimuli vs time and the output is response vs time curve. Depending on how we train the network the response curve could represent average response to the stimuli from different spatial locations or could represet response form a single spatial location. \n",
    "\n",
    "This notebook builds an LSTM based model to predict the response to external stimuli. To do this, we'll provide the model with timeseries of stimuli and use the return sequences to return an output vector that has the same shape as that of the input time series, the output represents response to the stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras, tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError (\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='same')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input csv file contains N columns of response as tabular data, we load the data from disk and read it to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename='../images/data_style.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/ubuntu/stimuli_response/')\n",
    "files = list(data_dir.glob('*.csv'))\n",
    "stimuli = []\n",
    "response_roi = []\n",
    "response = {}\n",
    "model_name = []\n",
    "common_name = 'ROI'\n",
    "sliding_window = 5\n",
    "for count, file in enumerate(files):\n",
    "    \n",
    "  stimuli_data = pd.read_csv(file, header=None)\n",
    "  \n",
    "  stimuli_data_tuple = stimuli_data.to_numpy()\n",
    "  stimuli_data_array = np.asarray(stimuli_data_tuple)\n",
    "  stimuli.append(stimuli_data_array[:,0])\n",
    "  for i in range(1, stimuli_data_array.shape[1]):\n",
    "    if common_name + str(i) in response.keys():\n",
    "      response_list = response[common_name + str(i)]\n",
    "      response_list.append(smooth(stimuli_data_array[:,i], window_len=sliding_window)) \n",
    "    else:  \n",
    "      response[common_name + str(i)] = [smooth(stimuli_data_array[:,i], window_len=sliding_window)]\n",
    "  \n",
    "stimuli = np.asarray(stimuli)\n",
    "stimuli = (stimuli - stimuli.mean()) / stimuli.std()\n",
    "#This is our input data for training\n",
    "stimuli = np.reshape(stimuli, (count + 1, stimuli_data_array.shape[0], 1 ) )\n",
    "# Now we create a dictionary of the target whose key is the name of the model and value is the training label\n",
    "for (k,v) in response.items():\n",
    "    varr = np.asarray(v)\n",
    "    response[k] = varr\n",
    "    print('Model Name', k, 'Output data shape', varr.shape)\n",
    "print('Input data shape', stimuli.shape)    \n",
    "\n",
    "example_plot = 4\n",
    "plt.plot(np.arange(stimuli.shape[1]), stimuli[example_plot,:,0])\n",
    "plt.title('Stimuli'+  str(example_plot))\n",
    "plt.show()\n",
    "plt.plot(np.arange(stimuli.shape[1]),response[list(response.keys())[8]][example_plot,:])\n",
    "plt.title('Response' + str(list(response.keys())[8]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (stimuli.shape[1],stimuli.shape[2])\n",
    "units = 64\n",
    "learning_rate = 0.01\n",
    "epochs = 150\n",
    "batch_size = 4\n",
    "validation_split = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 4 colums of the stimuli and the first number represents the number of timepoints present in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.layers import ConvLSTM1D, LSTM, Conv1D, Dense, UpSampling1D, Bidirectional, Conv1D,Dropout,Concatenate, BatchNormalization, MaxPooling1D\n",
    "inputs = layers.Input(shape = input_shape)\n",
    "#First Layer\n",
    "conv1 = LSTM(units = units,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' ) (inputs)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "conv1 = LSTM(units = units,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' ) (conv1)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "pool1 = MaxPooling1D(pool_size=2) (conv1)\n",
    "\n",
    "#Second Layer\n",
    "\n",
    "conv2 = LSTM(units = units * 2,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(pool1)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "conv2 = LSTM(units = units * 2,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv2)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "\n",
    "#Third Layer\n",
    "\n",
    "conv3 = LSTM(units = units * 4,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(pool2)\n",
    "conv3 = BatchNormalization()(conv3)\n",
    "conv3 = LSTM(units = units * 4,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv3)\n",
    "conv3 = BatchNormalization()(conv3)\n",
    "pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "\n",
    "#Fourth Layer\n",
    "\n",
    "conv4 = LSTM(units = units * 8,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(pool3)\n",
    "conv4 = BatchNormalization()(conv4)\n",
    "conv4 = LSTM(units = units * 8,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv4)\n",
    "conv4 = BatchNormalization()(conv4)\n",
    "\n",
    "#Dropout\n",
    "\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling1D(pool_size=2)(drop4)\n",
    "\n",
    "\n",
    "#Fifth Layer\n",
    "\n",
    "conv5 = LSTM(units = units * 16,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(pool4)\n",
    "conv5 = BatchNormalization()(conv5)\n",
    "conv5 = LSTM(units = units * 16,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv5)\n",
    "conv5 = BatchNormalization()(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "#Upsampling Layers\n",
    "\n",
    "up6 = LSTM(units = units * 8,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(UpSampling1D(size = 2)(drop5))\n",
    "merge6 = Concatenate(axis=-1)([conv4,up6])\n",
    "conv6 = LSTM(units = units * 8,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(merge6)\n",
    "conv6 = BatchNormalization()(conv6)\n",
    "conv6 = LSTM(units = units * 8,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv6)\n",
    "conv6 = BatchNormalization()(conv6)\n",
    "\n",
    "up7 = LSTM(units = units * 4,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(UpSampling1D(size = 2)(conv6))\n",
    "merge7 = Concatenate(axis=-1)([conv3,up7])\n",
    "conv7 = LSTM(units = units * 4,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(merge7)\n",
    "conv7 = BatchNormalization()(conv7)\n",
    "conv7 = LSTM(units = units * 4,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv7)\n",
    "conv7 = BatchNormalization()(conv7)\n",
    "\n",
    "up8 = LSTM(units = units * 2,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(UpSampling1D(size = 2)(conv7))\n",
    "merge8 = Concatenate(axis=-1)([conv2,up8])\n",
    "conv8 = LSTM(units = units * 2,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(merge8)\n",
    "conv8 = BatchNormalization()(conv8)\n",
    "conv8 = LSTM(units = units * 2,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv8)\n",
    "conv8 = BatchNormalization()(conv8)\n",
    "\n",
    "up9 = LSTM(units = units,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(UpSampling1D(size = 2)(conv8))\n",
    "merge9 = Concatenate(axis=-1)([conv1,up9])\n",
    "conv9 = LSTM(units = units,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(merge9)\n",
    "conv9 = BatchNormalization()(conv9)\n",
    "conv9 = LSTM(units = units,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv9)\n",
    "conv9 = BatchNormalization()(conv9)\n",
    "conv9 = LSTM(units = units,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv9)\n",
    "conv9 = BatchNormalization()(conv9)\n",
    "conv10 = LSTM(units = 1,  activation = 'relu', return_sequences=True, kernel_initializer = 'he_normal' )(conv9)\n",
    "outputs = conv10\n",
    "model = models.Model(inputs, outputs)\n",
    "opt = keras.optimizers.RMSprop(lr=learning_rate)\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=opt, metrics = ['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plot the network graph using keras plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "model_inline = plot_model(model, to_file = './model_time_series.png', \n",
    "show_shapes = True, show_layer_names=True)\n",
    "\n",
    "print(model_inline)\n",
    "model_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  plt.figure()\n",
    "  plt.plot(history.epoch, np.array(history.history['mae']),\n",
    "           label='Train Loss')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_mae']),\n",
    "           label = 'Val loss')\n",
    "  plt.legend()\n",
    "  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start the model training by first defining some parameters like epochs, validation_split, keras callbacks etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "for (k,v) in response.items():\n",
    "        model_name = k \n",
    "        # Keras callbacks\n",
    "        lrate = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=4, verbose=0)\n",
    "        srate = callbacks.ModelCheckpoint(data_dir.as_posix() +  '/models/' + model_name + '.h5', monitor='loss', verbose=1,\n",
    "                                          save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        history = model.fit(stimuli, v,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose = True,\n",
    "                    validation_split= validation_split,\n",
    "                    shuffle=True,\n",
    "                               callbacks=[lrate, srate])\n",
    "        plot_history(history) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNET usual but in 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.layers import ConvLSTM1D, LSTM, Conv1D, Dense, UpSampling1D, Bidirectional, Conv1D,Dropout,Concatenate, BatchNormalization, MaxPooling1D\n",
    "inputs = layers.Input(shape = input_shape)\n",
    "#First Layer\n",
    "conv1 = Conv1D(units, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal' ) (inputs)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "conv1 = Conv1D(units, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal' ) (conv1)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "pool1 = MaxPooling1D(pool_size=2) (conv1)\n",
    "\n",
    "#Second Layer\n",
    "\n",
    "conv2 = Conv1D(units * 2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "conv2 = Conv1D(units * 2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "\n",
    "#Third Layer\n",
    "\n",
    "conv3 = Conv1D(units * 4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = BatchNormalization()(conv3)\n",
    "conv3 = Conv1D(units * 4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "conv3 = BatchNormalization()(conv3)\n",
    "pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "\n",
    "#Fourth Layer\n",
    "\n",
    "conv4 = Conv1D(units * 8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = BatchNormalization()(conv4)\n",
    "conv4 = Conv1D(units * 8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "conv4 = BatchNormalization()(conv4)\n",
    "\n",
    "#Dropout\n",
    "\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling1D(pool_size=2)(drop4)\n",
    "\n",
    "\n",
    "#Fifth Layer\n",
    "\n",
    "conv5 = Conv1D(units * 16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = BatchNormalization()(conv5)\n",
    "conv5 = Conv1D(units * 16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "conv5 = BatchNormalization()(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "#Upsampling Layers\n",
    "\n",
    "up6 = Conv1D(units * 8, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling1D(size = 2)(drop5))\n",
    "merge6 = Concatenate(axis=-1)([conv4,up6])\n",
    "conv6 = Conv1D(units * 8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = BatchNormalization()(conv6)\n",
    "conv6 = Conv1D(units * 8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "conv6 = BatchNormalization()(conv6)\n",
    "\n",
    "up7 = Conv1D(units * 4 , 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling1D(size = 2)(conv6))\n",
    "merge7 = Concatenate(axis=-1)([conv3,up7])\n",
    "conv7 = Conv1D(units * 4 , 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = BatchNormalization()(conv7)\n",
    "conv7 = Conv1D(units * 4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "conv7 = BatchNormalization()(conv7)\n",
    "\n",
    "up8 = Conv1D(units  * 2, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling1D(size = 2)(conv7))\n",
    "merge8 = Concatenate(axis=-1)([conv2,up8])\n",
    "conv8 = Conv1D(units  * 2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = BatchNormalization()(conv8)\n",
    "conv8 = Conv1D(units  * 2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "conv8 = BatchNormalization()(conv8)\n",
    "\n",
    "up9 = Conv1D(units, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling1D(size = 2)(conv8))\n",
    "merge9 = Concatenate(axis=-1)([conv1,up9])\n",
    "conv9 = Conv1D(units, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = BatchNormalization()(conv9)\n",
    "conv9 = Conv1D(units, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = BatchNormalization()(conv9)\n",
    "conv9 = Conv1D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = BatchNormalization()(conv9)\n",
    "conv10 = Conv1D(1, 1, activation = 'linear')(conv9)\n",
    "outputs = conv10\n",
    "model = models.Model(inputs, outputs)\n",
    "opt = keras.optimizers.RMSprop(lr=learning_rate)\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=opt, metrics = ['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e140276ae15c1be7b597ddfede76c3757c35dc6a4240f18994000df39384733e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
